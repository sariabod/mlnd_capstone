{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.structured import *\n",
    "from fastai.column_data import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load the data\n",
    "df = pd.read_csv('course_ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets remove high rating rows\n",
    "joined = df.drop(df[df['rating'] > 5].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define catigorical vars\n",
    "cat_vars = ['user','course','category','job','institution','state']\n",
    "#coninuous vars\n",
    "cont_vars = ['rating']\n",
    "\n",
    "# df length\n",
    "n = len(df)\n",
    "\n",
    "# test percentage\n",
    "testp = n*.2\n",
    "\n",
    "#lets update features, categoricial should be category and continuous should be float32\n",
    "for v in cat_vars: \n",
    "    joined[v] = joined[v].astype('category').cat.as_ordered()\n",
    "    \n",
    "for v in cont_vars:\n",
    "    joined[v] = joined[v].fillna(0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to convert all category variables into contiguous ones and save the look up table\n",
    "lookup_table = {}\n",
    "for c in cat_vars:\n",
    "    uq = joined[c].unique()\n",
    "    col2idx =  {o:i for i,o in enumerate(sorted(uq))}\n",
    "    lookup_table[c] = col2idx\n",
    "    joined[c] = joined[c].apply(lambda x: col2idx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test/train set split\n",
    "joined, joined_test = train_test_split(joined, test_size=0.25)\n",
    "df = joined.drop('rating', axis=1)\n",
    "y = joined['rating']\n",
    "df_test = joined_test.drop('rating', axis=1)\n",
    "\n",
    "# create a validation set of ids\n",
    "train_ratio = 0.75\n",
    "train_size = int(len(df) * train_ratio)\n",
    "val_idx = list(range(train_size, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>course</th>\n",
       "      <th>category</th>\n",
       "      <th>job</th>\n",
       "      <th>institution</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41048</th>\n",
       "      <td>51063</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46996</th>\n",
       "      <td>41967</td>\n",
       "      <td>174</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143447</th>\n",
       "      <td>32505</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108864</th>\n",
       "      <td>40033</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118686</th>\n",
       "      <td>35442</td>\n",
       "      <td>171</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15173</th>\n",
       "      <td>51129</td>\n",
       "      <td>172</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7088</th>\n",
       "      <td>45562</td>\n",
       "      <td>172</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155733</th>\n",
       "      <td>26602</td>\n",
       "      <td>174</td>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33243</th>\n",
       "      <td>50147</td>\n",
       "      <td>71</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134300</th>\n",
       "      <td>43883</td>\n",
       "      <td>179</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user course category job institution state\n",
       "41048   51063     61       11  29          11    30\n",
       "46996   41967    174        7   6          11     5\n",
       "143447  32505     28        7   6          11    41\n",
       "108864  40033     70        0  28          11    30\n",
       "118686  35442    171        7   6           9    30\n",
       "15173   51129    172        7   6           0    24\n",
       "7088    45562    172        7   6          14    24\n",
       "155733  26602    174        7  44          14    30\n",
       "33243   50147     71        8   6          11    24\n",
       "134300  43883    179        7   6           8    39"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final df , everything is numeric and continguous\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets build the initial model\n",
    "md = ColumnarModelData.from_data_frame(\"models/\", val_idx, df, y.astype(np.float32), cat_flds=cat_vars, bs=128, test_df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(52118, 50), (220, 50), (15, 8), (46, 23), (16, 8), (61, 31)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build embedding matrix, matrices should be 1 bigger then the the number of categorical options to leave room for unknown\n",
    "cat_sz = [(c, len(joined[c].cat.categories)+1) for c in cat_vars]\n",
    "emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]\n",
    "emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the learner, \n",
    "# .04 dropout on the embedding matrix\n",
    "# 1000 and 500 nodes on 2 different layers\n",
    "# .001 and .01 dropouts\n",
    "# specifying the range of y (rating) to be 0-5\n",
    "m = md.get_learner(emb_szs,0 ,0.04, 1, [1000,500], [0.001,0.01],y_range=(0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.cuda.FloatTensor\u001b[0m), but expected (torch.cuda.FloatTensor source, int dim, torch.cuda.LongTensor index)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4c5d74d34759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/paperspace/mlnd_capstone/venv/lib/python3.6/site-packages/fastai/column_data.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_crit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_reg\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_multi\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrn_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrn_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paperspace/mlnd_capstone/venv/lib/python3.6/site-packages/fastai/model.py\u001b[0m in \u001b[0;36mmodel_summary\u001b[0;34m(m, input_size)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0min_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paperspace/mlnd_capstone/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paperspace/mlnd_capstone/venv/lib/python3.6/site-packages/fastai/column_data.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_cat, x_cont)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_emb\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paperspace/mlnd_capstone/venv/lib/python3.6/site-packages/fastai/column_data.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_emb\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paperspace/mlnd_capstone/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paperspace/mlnd_capstone/venv/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paperspace/mlnd_capstone/venv/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, indices, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.cuda.FloatTensor\u001b[0m), but expected (torch.cuda.FloatTensor source, int dim, torch.cuda.LongTensor index)"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80312aaec85849ab8962dcf61a40541d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 622/771 [00:05<00:01, 110.22it/s, loss=1.22] \n",
      "                                                             \r"
     ]
    }
   ],
   "source": [
    "#lets find a learning rate\n",
    "m.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEOCAYAAABmVAtTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd8XNWZ//HPM+pdsrolS7ZsucgNywKbbpYS05tDCZAeAixZkmxISLKbhJRd8mMhS69J2BAgoSTE9G4MuOFesSXLlrtkyaqWVef5/TEjRRhLGgmN7ozmeb9e89LMnTszX13LenTuOfccUVWMMcYYAJfTAYwxxgQOKwrGGGO6WVEwxhjTzYqCMcaYblYUjDHGdLOiYIwxppsVBWOMMd2sKBhjjOlmRcEYY0w3KwrGGGO6hTsdYKDS0tJ07NixTscwxpigsmrVqmpVTe9vv6ArCmPHjmXlypVOxzDGmKAiIhW+7Genj4wxxnSzomCMMaabFQVjjDHdrCgYY4zpZkXBGGNMNysKxhhjuoVMUahqbOGV9ftpae90OooxxgSskCkKK3Yc4l+fXs32g01ORzHGmIAVMkWhMCMBgLIqKwrGGNObkCkKY9NiCXOJFQVjjOlDyBSFqPAw8kfFUlppRcEYY3oTMkUBYEJGPGXWp2CMMb3yW1EQkT+ISJWIbOzl+WtEZL2IbBCRJSIy019ZukzIiGdn9WHaOtz+/ihjjAlK/mwpPAHM7+P5HcDpqjod+BXwqB+zAFCYGU+HW6moOezvjzLGmKDkt6KgqouBQ308v0RVa70PlwG5/srSxUYgGWNM3wKlT+EbwGv+/pCC9DgASq0oGGOCTGvH8Fx46/giOyJyBp6icEof+1wPXA+Ql5c36M+KjQwnNyXGWgrGmIC2r+4IL6/fx9tbqqhqaOGqE/J4ZsUurpmTx/WnjffrZztaFERkBvA4cK6q1vS2n6o+irfPoaSkRD/PZ07IiLeWgjEmYNUfaeei+z+kuqmNyVkJuES447VPAJiUlej3z3esKIhIHvA34DpV3TZcn1uYEc+S7TV0upUwlwzXxxpjTJ+eX7WHv6zYxZ7aI9QcbuOFG09idn4KAMvLa6hsbOX0if0usfy5+a0oiMgzwDwgTUT2AD8HIgBU9WHgZ0Aq8KCIAHSoaom/8nQpzEigrcPNntpm8lPj/P1xxhhzTIdbO3h7SyVvba5k6fYaag63MSkzgamjE/nVCdO6CwLAnILUYcvlt6Kgqlf38/w3gW/66/N7Mz4jHoDSyiYrCsYYR2ze18D3n13LJwcau7edOTmDR79c4vgZDMc7mofbhK6iUNXEWUWZDqcxxoSa3YeaueKRpagqdy6YwejkGCobWjirKNPxggAhWBSSYiLISIiyEUjGmGG1rbKRF1bv4d0tVQjw2ndPY8yoWKdjfUbIFQXwXNlcVtXY/47GGDMEDtS3cPlDS2hs6SAhOpw7vzgzIAsChGpRyEjguZW7UVW8ndzGGDPkOt3Kn5bu5PaXNhMd4eKdfz+dcalxuALgNFFvQrIojM+I53BbJ/vrWxidHON0HGPMCLRxbz03PbWaXYeaAfjR/MmMT493OFX/QrIoFPbobLaiYIwZSqrKku01fOeZNUSHu3jwmmJOm5hOfFRw/LoNlLmPhlXXCCTrbDbGDLX/eXMr1zy+HJfA09+ay3nTs4OmIECIthRS4yJJiY2wzmZjzJBQVVra3XxQepAHF23n1MI07lwwk6ykaKejDVhIFgURoTAjwVoKxpjPraqxhaseWUZ5tWedlgkZ8Txy3WxiI4Pz12twph4C4zPieXXDfhuBZIwZtLW767j56dVUNbZy3dx8CjPjuaJkDNERYU5HG7SQLQqFGfHUH2mnuqmN9IQop+MYY4LMqoparvv9ckbFRfL7r5RwaqH/J6sbDiFbFHp2NltRMMYM1IPvlREfFc7fbjyJjMTg6zvoTUiOPgLPVc0ApdbZbIwZoOqmVj4orebCmaNHVEGAEG4pZCVGkxAdzrZKKwrGGN+oKsvKD/HI4u0AXH3CGIcTDb2QLQoiwqTMBLYdsBFIxpj+ud3KD55bx9/W7AXg+tMKmJCR4HCqoReyp48AJmYlsLWyEdXPtcKnMSYEvLR+H39bs5esxGhiIsK4bm6+05H8ImRbCgCTMhN4+sguKhtag/IiE2PM8Fi4bh+3Pr+eiZnxvH7LaSgExNoH/hDaRSHL0/TbWtloRcEY8xmHDrfxny9u5NWN+ynOS+HuK2YG9AynQyGki8LETE9R2HagcVgWxDbGBIfGlnZW7DjEsyt388amSuZNSuf+LxUH1RxGgzXyv8M+jIqLJD0h6lPrpBpjQltjSzuXPPAR2w96pq244fTx3HbuZIdTDZ+QLgoAk7MSbFiqMabb794qpbz6MA9eU8yJBamkxEU6HWlYhfToI/CcQiqtaqTTbSOQjAl1dc1t/HlZBV+cnct507NDriCAFQUmZSbQ0u7uXh3JGBO6nliyk7ZON18+cazTURxjRaFrBJL1KxgT0paX1/C/b5dydlEmU0cnOh3HMSFfFLrmQLJ+BWNC273vlpIWH8V9V88K6en0Q74oxEaGkzcq1loKxoSw97cd5KOyGr59WkFQr4UwFEK+KIDnFNJWaykYE5Lqmtu46c+rmJARz5fm5Dkdx3FWFPB0Nu+oPkxrR6fTUYwxw8jtVn77+lYOt3Vy39WziAuBi9P6Y0UBT0uh061srzrsdBRjzDD6/Yc7eGbFLq4sGcOU7NDtXO7JigL/HIFknc3GhI7F2w5yx+ufcHZRJndcPt3pOAHDigIwLi2OyDAXW/Y3OB3FGDMMWjs6ue2F9RSkxfG7K48L6dFGR7OiAESEuSganciaXXVORzHGDIP3PjnIvvoWfnLelJCY5G4grCh4zcpLZv3eOto73U5HMcb42T/W7iUtPpJTC9OcjhJw/FYUROQPIlIlIht7eV5E5F4RKROR9SJS7K8svijOS6Gl3W3XKxgzwjW0tPPOJ1VcMGM04WH2d/HR/HlEngDm9/H8uUCh93Y98JAfs/RrVl4yAKt31ToZwxjjZ08uraCtw82ls3KcjhKQ/FYUVHUxcKiPXS4G/qQey4BkEcn2V57+5CTHkJ4QZf0KxoxgDy4q4843tnL82BRmjkl2Ok5AcrLtlAPs7vF4j3ebI0SE4rxk1lhLwZgRaUf1YR5etJ381FjuvuI4p+MErKA4oSYi14vIShFZefDgQb99zqy8FHbWNFPT1Oq3zzDGDL+W9k6+8ocViAgPXlPMmFGxTkcKWE4Whb3AmB6Pc73bPkNVH1XVElUtSU/331rKs7zNybW77RSSMSPJ/e+WsetQMw9dW8zU0UlOxwloThaFhcCXvaOQ5gL1qrrfwTzMyE0mzCXWr2DMCFJW1cgji7dz2awcThpvQ1D747erNkTkGWAekCYie4CfAxEAqvow8CpwHlAGNANf81cWX8VEhjElO8FGIBkzgvzu7VKiI8L4yflTnI4SFPxWFFT16n6eV+Bf/fX5g1Wcl8ILq/bQ6VbCXHbpuzHBrLKhhTc2HuCrJ40lLT7K6ThBISg6mofTrLxkDrd1UlplF7EZE+wefn87Clx3Yr7TUYKGFYWjzBqTAsDqCutXMCaYHahv4anlu7i8OIf81Din4wQNKwpHyU+NZVRcpF2vYEyQe2hRGW638p1/KXQ6SlCxonAUEWHWmGTW2LBUY4LWy+v38dTyXSyYnWvXJAyQFYVjmJWXTFlVE/XN7U5HMcYMkNut3PnGVgozE2zE0SBYUTiG4jxPv8LaPdZaMCbYLNtRQ0VNM98+rYDE6Ain4wQdKwrHMGNMMiJYv4IxQegvK3aTGB3O/GlZTkcJSlYUjiE+KpxJmQmstiubjQkqH5VVs3DdPq46IY/oiDCn4wSlfouCiMSJiMt7f6KIXCQiI75NVpyfwpqKWjrd6nQUY4wPVD19CbkpMXz/7IlOxwlavrQUFgPRIpIDvAlch2cBnRHthLGjaGztYMv+BqejGGN8sGR7DWt31/Ht08dbK+Fz8KUoiKo2A5cBD6rqF4Gp/o3lvBPGjQJgWXmNw0mMMf1RVe55u5SsxGiuKMl1Ok5Q86koiMiJwDXAK95tI74Mj06OYVJmAm9trnQ6ijGmHx/vrGXFzkPcOG88UeEj/teTX/lSFL4L/Bj4u6puEpEC4D3/xgoMZxdl8vHOQ9QfsesVjAlkTy+vICE6nCtKxvS/s+lTv0VBVd9X1YtU9bfeDudqVf23YcjmuJPGp+JWWF1hQ1ONCVT1ze28uvEAl87KISbSWgmfly+jj54WkUQRiQM2AptF5Fb/R3PerLwUwl3C8h2HnI5ijOnFP9btpa3DzZXHWythKPhy+qhIVRuAS4DXgHF4RiCNeDGRYczITeLjnVYUjAlU735SRUFanC2zOUR8KQoR3usSLgEWqmo7EDKD948fN4r1e+o40tbpdBRjzFHaO92s2HGIkyfYMptDxZei8AiwE4gDFotIPhAyg/fnjBtFe6eyZrf1KxgTaP60tILmtk7+ZXKG01FGDF86mu9V1RxVPU89KoAzhiFbQOiaHM86m40JLCt3HuK/Xt3CmZMzmDcp3ek4I4YvHc1JInK3iKz03u7C02oICcmxkRRmxLPKioIxAeWPH+0kMTqce66ehYitpz5UfDl99AegEbjCe2sA/ujPUIGmZGwKqypqcds8SMYEhLrmNt7aXMnFx+UQHxXudJwRxZeiMF5Vf66q5d7b7UCBv4MFkuK8FBpaOig72OR0FGMM8I+1+2jrdPNFm9JiyPlSFI6IyCldD0TkZOCI/yIFnpKxnnmQ7BSSMYHhva1VFKTbMFR/8KUo3Ag8ICI7RaQCuB+4wb+xAsvY1FhS4yJZudOKgjFOc7uV1RW1nOD9Y80MrX5PxqnqWmCmiCR6H4fMcNQuIkJxfgqrbSU2Yxy3tbKRhpYOZuenOB1lROq1KIjI93vZDoCq3u2nTAGpJD+FtzZXUtXYQkZCtNNxjAlZC9ftI8wlnG7DUP2ir9NHCf3cQspJ4z1XTC4ps/UVjHFKVWMLz6zYxekT0+2PMz/ptaXgHWVkvKaOTiQlNoLFpQe5ZFaO03GMCTmdbuWGJ1dxpK2Tn5w32ek4I5YvHc0GcLmEUwrT+bC0GlW7XsGY4bZ2dy2rd9Xx8wunMiEj5E5WDBsrCgNwamEaVY2tbKu06xWMGW5vbqokIky4YGa201FGNCsKA3Bqoadf4YPSgw4nMSa0qCpvbq5kbkEqidERTscZ0fodkioiUcDlwNie+6vqL/0XKzBlJ8UwISOexaXVfPPUkLqo2xhHbT/YxI7qw3z95LFORxnxfGkp/AO4GOgADve4haRTC9NYXl5DS7utr2DMcHljUyUAZxVlOpxk5PNlJqlcVZ0/mDcXkfnAPUAY8Liq3nHU83nA/wHJ3n1uU9VXB/NZw+W0wnT++NFOVlXU2sIexgyTtzZXMiM3ieykGKejjHi+tBSWiMj0gb6xiIQBDwDnAkXA1SJSdNRu/wE8q6qzgKuABwf6OcNtTsEoIsKExdavYMywqGxoYe3uOs6xVsKw8KUonAKsEpGtIrJeRDaIyHofXncCUOadWbUN+Aue01A9KZDovZ8E7PM1uFNiI8OZnZ/CB9uqnY5iTEh4aZ3n18LZRVkOJwkNvpw+OneQ750D7O7xeA8w56h9fgG8KSLfwbNwz1mD/KxhdWphOne+sZWqhhYyEu2qSmP8pbqplbvf2saJBalMzIx3Ok5I8GU5zgo85/wv9N6SvduGwtXAE6qaC5wHPCkin8kkItd3rfx28KDzp23mT/P8xfLcqj0OJzFmZHt+1R6a2zr55cVTbXW1YeLLcpy3AE8BGd7bn71/2fdnLzCmx+Nc77aevgE8C6CqS4Fo4DO9t6r6qKqWqGpJerrzk2CNT4/nhHGj+Mfao78dY8xQemdLJTNzkyjMtCuYh4svfQrfAOao6s9U9WfAXOBbPrzuY6BQRMaJSCSejuSFR+2zCzgTQESm4CkKzjcFfHDm5Ay2VTZR2dDidBRjRiRVZeuBRqbl2EI6w8mXoiBAz0H5nd5tfVLVDuBm4A1gC55RRptE5JcicpF3t38HviUi64BngK9qkEwsdGqhp8XyQal1OBvjD1WNrTS0dDDRWgnDypeO5j8Cy0Xk797HlwC/9+XNvdccvHrUtp/1uL8ZONm3qIFlclYCafGRfFh6kAWzbZ1YY4batspGAAqtg3lY+bLy2t0isgjP0FSAr6nqGr+mCgIul3DKhDQ+LKvG7VZcLusEM2Yord9TD0BRdmI/e5qh1Ovpo67lN0VkFLAT+LP3VuHdFvJOn5ROdVMbG/bWOx3FmBFnVUUtEzLiSY6NdDpKSOmrpfA0cAGwCs9FZl3E+zjkZ4SbNzEDl3hHSIxJdjqOMSNGS3snH+88xHnTbJrs4dZrS0FVL/B+HaeqBT1u41Q15AsCQEpcJCX5o3h7S5XTUYwZUV5cs5fGlg5b5dABvlyn8I4v20LVmVMy2Ly/gb11R5yOYsyI8dbmSvJTY5lbYGeqh1tffQrR3r6DNBFJEZFR3ttYPFNYGODMKZ5Jut7dUulwEmNGhk63smLnIU4sSLWrmB3QV0vh23j6EyZ7v3bd/gHc7/9owWF8ehxjU2PtFJIxQ+STAw00tnQwx1oJjuirT+EeVR0H/KBHX8I4VZ2pqlYUvESEc6ZmsWR7NfVH2p2OY0zQW15+CIA541IdThKafJkQ7z4RmSYiV4jIl7tuwxEuWJw7LYv2TuUdO4VkzOe2YschclNiGJ1sC+o4wZeO5p8D93lvZwD/D7iozxeFmOPGJDM6KZpXNxxwOooxQW/T/nqOsyHejvFl7qMFeCatO6CqXwNm4lkQx3iJCPOnZbN420EaW+wUkjGD1dLeyZ7aI0zIsKktnOJLUTiiqm6gw3uVcxWfnhLbAOdNz6Kt0827n1iHszGDtaP6MKqe6emNM3wpCitFJBl4DM/oo9XAUr+mCkLFeSlkJkbxyvr9TkcxJmiVVTUBVhSc5MuEeDd57z4sIq8DiarqyxrNIcXlEs6dls3TK3bR1NpBfJQvE9AaY3paufMQMRFhjM+IczpKyOrr4rXio2/AKCDce98c5bzp2bR12CkkYwbrg9Jq5haMIio8zOkoIauvP2fv8n6NBkqAdXgmw5sBrARO9G+04DM7P4X0hCheXb+fi2aOdjqOMUFl96FmyqsPc+3cfKejhLS+Ll47Q1XPAPYDxd41kmcDs/jsWssGCHMJ50/P5t2tVdQ1tzkdx5ig0rWK4WkTnV+HPZT50tE8SVU3dD1Q1Y3AFP9FCm5XlIyhrcPNsyt3Ox3FmKDy0fZqspOiGZ9u/QlO8qUorBeRx0Vknvf2GGAdzb0oGp3IyRNSeXRxOUfaOvt/gTEGgDUVtczOT7FJ8BzmS1H4GrAJuMV72+zdZnpx07wJVDe18bZNe2GMTw7Ut7CvvoVZeSlORwl5vgxJbQF+570ZH5xYkEpWYjT/WLuPC63D2Zh+rd5VC0Bxnk1v4bRei4KIPKuqV4jIBj69HCcAqjrDr8mCmMslXHTcaP740Q7qmttsjVlj+rG6opbIcBdTR9sMOk7r6/TRLd6vFwAXHuNm+nDJcTm0dyqPf7DD6SjGBLzVu2qZnpNEZLgvZ7SNP/U1JHW/92vFsW7DFzE4FY1O5LJZOTyyeDuVDS1OxzEmYB063Mba3XWcWGDrJwSCvq5obhSRhmPcGkWkYThDBqtbziqk0638aelOp6MYE7De3lKJW+ELU7OcjmLou6WQoKqJx7glqGricIYMVvmpccyblMHzq/bQ6f5Mt4wxBnhz0wFykmOYlmO/VgKBzyfwRCRDRPK6bv4MNZJcUTKGyoZW3txkC/AYc7QjbZ0sLq3m7KJMuz4hQPiy8tpFIlIK7ADeB3YCr/k514hxdlEm+amxPLy4HFVrLRjT05pdtbR1uDndprYIGL60FH4FzAW2qeo4PKuwLfNrqhEkzCV869QC1u2u46OyGqfjGBNQVuw8hAgU59tFa4HCl6LQrqo1gEtEXKr6Hp5ZU42PFszOZcyoGH764gaqbCSSMd0+KqumKDuRpJgIp6MYL1+KQp2IxAOLgadE5B7gsH9jjSzREWH875XHcbCxlZ/8fUP/LzAmBFQ1trCyopazizKdjmJ68KUoXAw0A98DXge2YxevDdjs/FF889QC3vmkip3VVlON+WBbNapwTpENRQ0kvhSFbwPZqtqhqv+nqvd6TyeZAbp2Th7hLuH/lu50Oooxjlu1q5aE6HAmZyU4HcX04EtRSADeFJEPRORmEbG23iBlJEZz4YzRPLNiF/vqjjgdxxhHrdpZS3FeCi6XDUUNJP0WBVW9XVWnAv8KZAPvi8jbvry5iMwXka0iUiYit/WyzxUisllENonI0wNKH4S+f85E3Ar3vlPqdBRjHFN/pJ1tVY3MtlFHAWcgs09VAQeAGiCjv51FJAx4ADgXKAKuFpGio/YpBH4MnOwtPN8dQJ6glJsSy1XHj+GF1XustWBC1ppdtahCiRWFgOPLxWs3icgi4B0gFfiWj9NmnwCUqWq5qrYBf8HTad3Tt4AHVLUWQFWrBhI+WF1/WgGq8NCi7U5HMcYRqypqcQnMHGPrJwQaX1oKY4DvqupUVf2Fqm728b1zgJ4LFe/xbutpIjBRRD4SkWUiMv9YbyQi14vIShFZefDgQR8/PnDlpsRy1QljeGp5BWt31zkdx5hht7z8ENNykoiL6nedLzPMfOlT+LGqrvXT54cDhcA84GrgMRH5zJ8Oqvqoqpaoakl6+si4HP7WcyaTnRTDvz2zho5Ot9NxjBk2Le2drN1dx5xxo5yOYo7Bnyta7MXTyuiS693W0x5goaq2q+oOYBueIjHiJcVG8IuLprLrUDMvr9/vdBxjhs3a3XW0dbqZM87WTwhE/iwKHwOFIjJORCKBq4CFR+3zIp5WAiKShud0UrkfMwWUMydnMCU7kV+/soWaplan4xgzLJaXe+Y7On6stRQCkd+Kgqp2ADcDbwBbgGdVdZOI/FJELvLu9gZQIyKbgfeAW0PpwjiXS7j7ipk0HGnnRy9ssFlUTUhYWXGISZkJJMXafEeByK8Loqrqq6o6UVXHq+pvvNt+pqoLvfdVVb+vqkWqOl1V/+LPPIFoSnYiPzp3Mm9vqeSh9200khn5SiubKBptC+oEKlslOwB8/eSxnD89m7vf3Maumman4xjjN40t7RxoaGF8erzTUUwvrCgEABHhZxcW4XIJ979nVzqbkav8oGcyyAkZVhQClRWFAJGZGM2XTsjjhdV7KT/Y5HQcY/xi074GwIpCILOiEEBuOmM8MRFh/OaVLU5HMcYv3tlSSU5yDAVpcU5HMb2wohBAMhKiuflfJvDOJ1W8tzUkZvwwIaSj081H26s5a0oGIjYzaqCyohBgvnbyWArS4/jPFzfS3NbhdBxjhsyO6sO0tLs5Ls/mOwpkVhQCTFR4GP996XT21B7hnret09mMHFsONAIwOcuGowYyKwoBaE5BKleU5PL7D3dQVmWdzmZk2LK/gYgwseGoAc6KQoD64fzJxESGcftLm+xKZzMibN7XwISMBCLD7ddOILN/nQCVFh/FD86ZxAel1bxkE+aZIKeqbNxbzzS7kjngWVEIYNfOzWdmbhK/fGkTDS3tTscxZtAqG1qpOdzGtJwkp6OYflhRCGBhLuE3l06n5nAbD75n8yKZ4LVpXz0AU62lEPCsKAS4aTlJXDorhz98tIPtdqWzCVIb9zYg4pkA0gQ2KwpB4NYvTCIuMoxrHltOVWOL03GMGbCN++opSIuz5TeDgBWFIJCdFMNT35xL3ZE2bn5qDe22fKcJMtsqG5lsrYSgYEUhSBSNTuSOy2awYuch7n3HLmozwaO1o5Pdh5oZb/MdBQUrCkHkklk5LJidywPvlbGsPGQWqDNBbvehZtwKBXbRWlCwohBkfnHRVMamxXHz02uobLD+BRP4utZQGGcthaBgRSHIxEeF8/C1s2lu6+Cmp1bT1mH9Cyawbav0zHk0Lt2KQjCwohCEJmYm8NvLZ7CqopZfvbzZ6TjG9Gnt7noK0uNIjI5wOorxgRWFIHXhzNF8+7QCnlxWwYtr9jodx5hjUlXW7anjuFybLjtYWFEIYj+cP5lZecn86uXN7K8/4nQcYz6juqmNg42tTLXpLYKGFYUgFuYSfnv5DFo73Nzw59V02PULJsB09SdMykxwOInxlRWFIDcxM4E7Lp/Out113PdumdNxjPmUrqIwMcuGowYLKwojwAUzRnPZrBzufbfU+hdMQNlW2URybATp8VFORzE+solIRojfXDqdPXVH+O5f16IolxyXY4ujG8eVVjYyMSPBfhaDiLUURoiYyDD+9PUTKMpO5Ht/XcdtL2ywFduMo1SVbZWNFGbaqaNgYkVhBImOCOOFG0/i26cX8NeVu7n9pc12cZtxTFVjKw0tHUy0TuagYqePRpiYyDBumz+ZlrZOnliykzW767jnyuMYa1MMmGHW1clsLYXgYi2FEUhEuP3iafzWOypp3v8s4o8f7XA6lgkx2yo9i0JZSyG4WEthBLvy+DwmZCTwwHtl3P7SZmIjw7jy+DynY5kQUVrZyKi4SNJs5FFQsZbCCDc7P4WHr53N6RPT+fHfNnDXm1vtIjczLLZVNlKYYaeOgo0VhRAQGe7ioWuLOXdaNve9W8bNT6+hua3D6VhmBFNVSqua7NRREPJrURCR+SKyVUTKROS2Pva7XERUREr8mSeUxUaG88A1xfz8wiJe33SAs+56nw176p2OZUaoAw0tNLZ0MNE6mYOO34qCiIQBDwDnAkXA1SJSdIz9EoBbgOX+ymL+6Wsnj+O5G05ERFjw8BKeXbnbrmcwQ66rk7nQWgpBx58thROAMlUtV9U24C/AxcfY71fAbwFbRmyYHD92FAtvPpnivBR++Px6fvriRj450EBjS7vT0cwIUdo155EVhaDjz9FHOcDuHo/3AHN67iAixcAYVX1FRG7t7Y1E5HrgeoC8PBs9MxRS46P48zfncMdrW3jsgx08vXwXSTERnD4xnZKxKVw3N9+mJjCDtq29jUtOAAAQMElEQVSykbT4SEbFRTodxQyQY0NSRcQF3A18tb99VfVR4FGAkpISO9cxRMJcwk/PL+Ly2bms3FnLU8t3sXDdPhau28e63fX8cP4kMhOjnY5pgtC2yiYm2MijoOTPorAXGNPjca53W5cEYBqwyPsXaRawUEQuUtWVfsxljjI5K5HJWYlcOzcft1u5662tPPx+Oa9s2MdN8yZQMjaF1nY3xfkpJMXYkoqmb6pKWVUTlxXnOB3FDII/i8LHQKGIjMNTDK4CvtT1pKrWA2ldj0VkEfADKwjOcrmEW78wmStL8vjly5u4+61t3c9lJETx9Lfm2l+Apk9Vja00tXbYz0mQ8ltRUNUOEbkZeAMIA/6gqptE5JfASlVd6K/PNp9fXmosj3/leA7Ut/DJgQZaO9z89O8b+OofV/DkN+YwzuZSMr3YX+8ZMzI6KcbhJGYw/NqnoKqvAq8ete1nvew7z59ZzOBkJUWTleTpV8hOiubLf1jB2Xe/z/TcJM6fns2phelMzIy3TmnTrbLBUxSsPyo42dxHxmczcpN5/ZbT+OOSHSwpq+HXr2wBtjAuLY4ffmESaQlRTMxMsH6HENddFJJszqNgZEXBDEhWUjQ/PncKALtqmllaXs3jH+zgxqdWA5AQFc5XTx7Lv54xgea2Tg63dpCdFE14mM2oEioO1LcQ7hLS4qwoBCMrCmbQ8lJjyUvN4/LiXF5YvYd9dS2UVjVy37tlvLJhP3trj9Da4SYpJoJTCtP4j/OnkG3nmUe0JWXVPLhoOwnR4bhcdkoxGFlRMJ9beJjrU1Nyv7npAD96YT3nTM1izrhRPL9qD6+s38/S7TX85pJpnDs928G0xl9+99Y27nmnFIDTCtMdTmMGS4Jt3puSkhJdudJGrQY6Vf1U53NpZSP//tw61u+ppyg7kdyUGM6aksmC2bmf+ouyrrmN+KhwO90UZDbtq+f8ez/ksuIc/uvS6URHhDkdyRxFRFapar+TjlpLwfjF0aORCjMTeOHGk3j8gx0sLa/hkwONvLm5kieW7GTepHQuPi6Hp5ZX8OSyCiakx/OfFxR1D3vNTYmx0U0B7sU1e4kIE352QZEVhCBnLQXjCFXlhdV7eeT97ZRXH6bT7fk5PLsok01769lX/8/5EY8bk8y1c/M5bWIaGQk2zDHQtHZ0cvId7zIrL4XHvmyz3wcqaymYgCYiLJidy4LZuVQ1tPDS+v2MT49j3qQMmts6eHXDAdxupe5IG48uLucHz60DYOaYZO5cMMNm3wwgf1pSQXVTG9fNzXc6ihkC1lIwAU9VWVVRy+pdtTzyfjn1R9oZmxbH5KwE/uP8ou6L68zw21PbzBn/s4jTJ2bw2Jdn22m+AOZrS8GKggkq1U2t3PN2KU8uq+jeNjM3ifnTsjlhXArhLhcTMuKJi7JGsC+OtHXS2NLOG5sr+WR/AzecPp7Vu2p5aNF2kmIiyBsVS/2RdtISothV00z5wSam5yZxWXEum/c18I+1e9lbd4T3bz2D0ck23DiQWVEwI1pzWwc1TW38fc1eXlm/n63eRV0AYiPDKMpO5AtTs1gwO5eUzzmnf3unm4gwF8+u3M2La/aSFBPByopaTp2Qxq3zJwXFtRd1zW2s3lXL21uqaOtwkxwTQdnBJpaU1dDW6f7Uvi6B0ckxZCREsbfOc61J/ZF2EqLCyU+NY/P+hu4+oKzEaH44fxKXFec68W2ZAbCiYEKGqlLV2MrS7TWIwLLyQ2zeV8+6PfWEuYQzJ2dwpL2ThOhw0uKjSImNpOun/qTxqdQ0tREeJnS6laXba3AJLNleA3iuzm1q62BCejylVU3ERYYhIszITWJpeQ0RLhcnTUhldHIM3zxlHAXpzs8M6nYrG/bW89rGA7R1uPl45yE27PWsxx0Z7iIyzEVzWwcF6fGcPjGd0ckxVDe1cnlxLm9uPkBZVRO3zZ9MRo+5i+qb24mKcBEdEUZTawdLyqoZnRzDtJwkp75NM0BWFEzIW1VxiNc2HGDhun0kx0ZwoL6FhpYOn147MzeJhpYOZuYmkZUUw/IdNczISeIn508hKtwz5HL7wSYeXrSdtbvrqDjUTEenm3mTMpidn8J1J+aTGH3sOaC2HmjkcFsHLW2dLNp2kI5OJTYyjOgIF3mpccyfmkVk+MCv0+jodPO/b5fy8vp97KxpJtwldKoydXQiZ0/JYkJGPGdOyQCgpb2T5FhbFS2UWFEw5hhqD7cRExnG7kPNJERHsHFvPRHhLppbO4iJDKMgLZ4xowZ+XcTBxlZ+/+EOnl+1m+qmNqLCXXzlpLFcfUIe2yobOWVCGqsqavnNK1s+daorIkxwidDa8c9TOCmxEWQmRhMeJsRGhFNefZi65jZm56fwpTl5XDBjNGEuYfvBJpZsr2H97jomZSXwzpYqlpbXMD0niSuPH8MFM7KJiwonwi4ENFhRMMYxa3bV8uSyCv6+Zi9d/72iI1y0tLvJGxXLZcU5FGYkUN3UyiWzckiKiaCuuY2o8DCWllfz2OIdhIcJIsLaXbUcl5fCxIx43ttaxfaDhwHPgkdVja0ARIa5aOt0Exnu4tcXT+OK48f0Fs2EMCsKxjhsV00zj39YTnxUOLXN7cRFhvG9sycOemSU2628smE/G/fVs2ZXHTNzk/hiyRgmpMezv6GFyDAX6Qk2M6k5NisKxhhjuvlaFOxkozHGmG5WFIwxxnSzomCMMaabFQVjjDHdrCgYY4zpZkXBGGNMNysKxhhjullRMMYY0y3oLl4TkYNABZAE1PeyW2/P+bq9v8dpQLWPkQejr+/t876mv/0Gcux82Tacx24wx20gr3Py2AXiz9xAXmf/Xwf3mqH8mStU1f6ntVXVoLwBjw70OV+3+/B4pVPf2+d9TX/7DeTY+bJtOI/dYI5bsBy7QPyZG6pjZ/9fnfuZO9YtmE8fvTSI53zd3t9jfxvM5/n6mv72G8ix82XbcB67wX6WHTtnj539fx38fp/3Z+4zgu70USAQkZXqwxwi5rPs2A2OHbfBs2M3MMHcUnDSo04HCGJ27AbHjtvg2bEbAGspGGOM6WYtBWOMMd2sKBhjjOlmRcEYY0w3Kwp+ICJxIrJSRC5wOkuwEJEpIvKwiDwvIjc6nSeYiMglIvKYiPxVRM5xOk8wEZECEfm9iDzvdJZAYUWhBxH5g4hUicjGo7bPF5GtIlImIrf58FY/Ap71T8rAMxTHTVW3qOoNwBXAyf7MG0iG6Ni9qKrfAm4ArvRn3kAyRMeuXFW/4d+kwcVGH/UgIqcBTcCfVHWad1sYsA04G9gDfAxcDYQB/33UW3wdmAmkAtFAtaq+PDzpnTMUx01Vq0TkIuBG4ElVfXq48jtpqI6d93V3AU+p6uphiu+oIT52z6vqguHKHsjCnQ4QSFR1sYiMPWrzCUCZqpYDiMhfgItV9b+Bz5weEpF5QBxQBBwRkVdV1e3P3E4biuPmfZ+FwEIReQUIiaIwRD9zAtwBvBYqBQGG7ufOfJoVhf7lALt7PN4DzOltZ1X9KYCIfBVPS2FEF4Q+DOi4eYvpZUAU8KpfkwW+AR074DvAWUCSiExQ1Yf9GS7ADfTnLhX4DTBLRH7sLR4hzYqCn6jqE05nCCaqughY5HCMoKSq9wL3Op0jGKlqDZ6+GONlHc392wuM6fE417vN9M2O2+DZsRs8O3afkxWF/n0MFIrIOBGJBK4CFjqcKRjYcRs8O3aDZ8fuc7Ki0IOIPAMsBSaJyB4R+YaqdgA3A28AW4BnVXWTkzkDjR23wbNjN3h27PzDhqQaY4zpZi0FY4wx3awoGGOM6WZFwRhjTDcrCsYYY7pZUTDGGNPNioIxxphuVhSM34lI0zB8xkU+Tms+lJ85T0ROGsTrZonI7733vyoi9w99uoETkbFHT0N9jH3SReT14cpkhp8VBRM0vNMiH5OqLlTVO/zwmX3NDzYPGHBRAH5CkM5VpKoHgf0iEjJrXoQaKwpmWInIrSLysYisF5Hbe2x/UURWicgmEbm+x/YmEblLRNYBJ4rIThG5XURWi8gGEZns3a/7L24ReUJE7hWRJSJSLiILvNtdIvKgiHwiIm+JyKtdzx2VcZGI/K+IrARuEZELRWS5iKwRkbdFJNM7ZfMNwPdEZK2InOr9K/oF7/f38bF+cYpIAjBDVdcd47mxIvKu99i8IyJ53u3jRWSZ9/v99bFaXuJZ7e8VEVknIhtF5Erv9uO9x2GdiKwQkQTv53zgPYarj9XaEZEwEbmzx7/Vt3s8/SJwzTH/gU3wU1W72c2vN6DJ+/Uc4FFA8PxB8jJwmve5Ud6vMcBGINX7WIErerzXTuA73vs3AY97738VuN97/wngOe9nFOGZXx9gAZ5puV1AFlALLDhG3kXAgz0ep/DPq/+/Cdzlvf8L4Ac99nsaOMV7Pw/Ycoz3PgN4ocfjnrlfAr7ivf914EXv/ZeBq733b+g6nke97+XAYz0eJwGRQDlwvHdbIp6ZkWOBaO+2QmCl9/5YYKP3/vXAf3jvRwErgXHexznABqd/ruzmn5tNnW2G0zne2xrv43g8v5QWA/8mIpd6t4/xbq8BOoEXjnqfv3m/rsKzBsOxvKietSw2i0imd9spwHPe7QdE5L0+sv61x/1c4K8iko3nF+2OXl5zFlAkIl2PE0UkXlV7/mWfDRzs5fUn9vh+ngT+X4/tl3jvPw38zzFeuwG4S0R+C7ysqh+IyHRgv6p+DKCqDeBpVQD3i8hxeI7vxGO83znAjB4tqSQ8/yY7gCpgdC/fgwlyVhTMcBLgv1X1kU9t9CywcxZwoqo2i8giPMuZArSoaudR79Pq/dpJ7z/DrT3uSy/79OVwj/v3AXer6kJv1l/08hoXMFdVW/p43yP883sbMqq6TUSKgfOAX4vIO8Dfe9n9e0AlnqVjXcCx8gqeFtkbx3guGs/3YUYg61Mww+kN4OsiEg8gIjkikoHnr9Bab0GYDMz10+d/BFzu7VvIxNNR7Isk/jkn/1d6bG8EEno8fhPPKmgAeP8SP9oWYEIvn7MEz1TP4Dln/4H3/jI8p4fo8fyniMhooFlV/wzcCRQDW4FsETneu0+Ct+M8CU8Lwg1ch2f94qO9AdwoIhHe1070tjDA07Loc5SSCV5WFMywUdU38Zz+WCoiG4Dn8fxSfR0IF5EteNYaXuanCC/gWZ5xM/BnYDVQ78PrfgE8JyKrgOoe218CLu3qaAb+DSjxdsxu5hgreqnqJ3iWzUw4+jk8BeVrIrIezy/rW7zbvwt837t9Qi+ZpwMrRGQt8HPg16raBlwJ3OftqH8Lz1/5DwJf8W6bzKdbRV0ex3OcVnuHqT7CP1tlZwCvHOM1ZgSwqbNNSOk6xy+etXlXACer6oFhzvA9oFFVH/dx/1jgiKqqiFyFp9P5Yr+G7DvPYuBiVa11KoPxH+tTMKHmZRFJxtNh/KvhLgheDwFfHMD+s/F0DAtQh2dkkiNEJB1P/4oVhBHKWgrGGGO6WZ+CMcaYblYUjDHGdLOiYIwxppsVBWOMMd2sKBhjjOlmRcEYY0y3/w8+qFbUfm0/OwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.sched.plot(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f9ca8d712242168c04d5384bf43e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      0.251065   0.231956  \n",
      "    1      0.178752   0.21344                                 \n",
      "    2      0.133683   0.214341                                \n",
      "    3      0.10436    0.216699                                 \n",
      "    4      0.082238   0.221261                                 \n",
      "    5      0.126928   0.235749                                 \n",
      "    6      0.096233   0.228888                                 \n",
      "    7      0.060322   0.232883                                 \n",
      "    8      0.034411   0.234541                                 \n",
      "    9      0.026437   0.239289                                 \n",
      "    10     0.051319   0.246238                                 \n",
      "    11     0.055719   0.236999                                 \n",
      "    12     0.02624    0.2341                                   \n",
      "    13     0.01419    0.23754                                  \n",
      "    14     0.009872   0.237952                                  \n",
      "    15     0.026616   0.244356                                 \n",
      "    16     0.036609   0.242393                                 \n",
      "    17     0.016266   0.235617                                 \n",
      "    18     0.00803    0.235991                                  \n",
      "    19     0.005839   0.236566                                  \n",
      "    20     0.017806   0.248948                                  \n",
      "    21     0.027397   0.239349                                 \n",
      "    22     0.012497   0.233382                                 \n",
      "    23     0.006227   0.234213                                  \n",
      "    24     0.004315   0.234015                                  \n",
      "    25     0.013121   0.238294                                  \n",
      "    26     0.021235   0.237722                                 \n",
      "    27     0.010917   0.233516                                 \n",
      "    28     0.005087   0.233629                                  \n",
      "    29     0.003804   0.233842                                  \n",
      "    30     0.009718   0.239142                                  \n",
      "    31     0.01725    0.234388                                 \n",
      "    32     0.009028   0.232109                                  \n",
      "    33     0.004319   0.232703                                  \n",
      "    34     0.00336    0.232304                                  \n",
      "    35     0.008024   0.238417                                  \n",
      "    36     0.015276   0.235326                                 \n",
      "    37     0.007707   0.231908                                  \n",
      "    38     0.00386    0.232084                                  \n",
      "    39     0.002834   0.231853                                  \n",
      "    40     0.007207   0.235693                                  \n",
      "    41     0.013077   0.235558                                 \n",
      "    42     0.006549   0.231564                                  \n",
      "    43     0.003652   0.230781                                  \n",
      "    44     0.002733   0.231167                                  \n",
      "    45     0.006537   0.234381                                  \n",
      "    46     0.011456   0.234593                                 \n",
      "    47     0.006455   0.233025                                  \n",
      "    48     0.003198   0.231042                                  \n",
      "    49     0.002586   0.230999                                  \n",
      "    50     0.005896   0.234341                                  \n",
      "    51     0.011111   0.23278                                   \n",
      "    52     0.00528    0.230815                                  \n",
      "    53     0.002947   0.230737                                  \n",
      "    54     0.002242   0.230605                                  \n",
      "    55     0.005693   0.231873                                  \n",
      "    56     0.009383   0.231026                                  \n",
      "    57     0.00503    0.230685                                  \n",
      "    58     0.002856   0.230491                                  \n",
      "    59     0.002314   0.23015                                   \n",
      "    60     0.004796   0.232036                                  \n",
      "    61     0.008124   0.231318                                  \n",
      "    62     0.004734   0.230452                                  \n",
      "    63     0.002789   0.230038                                  \n",
      "    64     0.002115   0.230224                                  \n",
      "    65     0.004925   0.232559                                  \n",
      "    66     0.007423   0.231827                                  \n",
      "    67     0.004429   0.230806                                  \n",
      "    68     0.002516   0.229908                                  \n",
      "    69     0.002179   0.229804                                  \n",
      "    70     0.004733   0.233874                                  \n",
      "    71     0.00711    0.232165                                  \n",
      "    72     0.004086   0.230349                                  \n",
      "    73     0.002534   0.230217                                  \n",
      "    74     0.002023   0.230033                                  \n",
      "    75     0.003863   0.231602                                  \n",
      "    76     0.006789   0.230429                                  \n",
      "    77     0.003756   0.229072                                  \n",
      "    78     0.002369   0.229115                                  \n",
      "    79     0.001779   0.229215                                  \n",
      "    80     0.003924   0.232734                                  \n",
      "    81     0.006166   0.230929                                  \n",
      "    82     0.003571   0.230072                                  \n",
      "    83     0.002225   0.229711                                  \n",
      "    84     0.001853   0.229631                                  \n",
      "    85     0.004101   0.229814                                  \n",
      "    86     0.005944   0.230975                                  \n",
      "    87     0.003353   0.228317                                  \n",
      "    88     0.002231   0.228827                                  \n",
      "    89     0.001809   0.228939                                  \n",
      "    90     0.003327   0.229755                                  \n",
      "    91     0.005513   0.231127                                  \n",
      "    92     0.003519   0.229678                                  \n",
      "    93     0.002267   0.229131                                  \n",
      "    94     0.001828   0.228753                                  \n",
      "    95     0.003152   0.230415                                  \n",
      "    96     0.005601   0.230228                                  \n",
      "    97     0.003088   0.228205                                  \n",
      "    98     0.00205    0.228445                                  \n",
      "    99     0.001608   0.228676                                  \n",
      "   100     0.003345   0.230467                                  \n",
      "   101     0.005207   0.228847                                  \n",
      "   102     0.002894   0.229606                                  \n",
      "   103     0.002042   0.229733                                  \n",
      "   104     0.001696   0.22928                                   \n",
      "   105     0.003286   0.231202                                  \n",
      "   106     0.004909   0.231344                                  \n",
      "   107     0.003153   0.229824                                  \n",
      "   108     0.001942   0.229352                                  \n",
      "   109     0.002006   0.229209                                  \n",
      "   110     0.002925   0.229923                                  \n",
      "   111     0.004702   0.230346                                  \n",
      "   112     0.002839   0.230102                                  \n",
      "   113     0.001882   0.229399                                  \n",
      "   114     0.001559   0.229192                                  \n",
      "   115     0.003202   0.230328                                  \n",
      "   116     0.004401   0.231473                                  \n",
      "   117     0.003049   0.231503                                  \n",
      "   118     0.001851   0.229405                                  \n",
      "   119     0.00158    0.22935                                   \n",
      "   120     0.002947   0.230149                                  \n",
      "   121     0.004445   0.229494                                  \n",
      "   122     0.002891   0.229484                                  \n",
      "   123     0.001855   0.228935                                  \n",
      "   124     0.001523   0.228624                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   125     0.002605   0.229087                                  \n",
      "   126     0.00414    0.229245                                  \n",
      "   127     0.003094   0.230052                                  \n",
      "   128     0.001843   0.229297                                  \n",
      "   129     0.001567   0.229079                                  \n",
      "   130     0.003467   0.233588                                  \n",
      "   131     0.004405   0.232756                                  \n",
      "   132     0.002716   0.23063                                   \n",
      "   133     0.001832   0.22959                                   \n",
      "   134     0.001606   0.229717                                  \n",
      "   135     0.002716   0.231145                                  \n",
      "   136     0.003637   0.230791                                  \n",
      "   137     0.002629   0.230705                                  \n",
      "   138     0.00189    0.229982                                  \n",
      "   139     0.001468   0.229555                                  \n",
      "   140     0.002544   0.232468                                  \n",
      "   141     0.003679   0.231182                                  \n",
      "   142     0.002561   0.229422                                  \n",
      "   143     0.001635   0.229158                                  \n",
      "   144     0.001427   0.229219                                  \n",
      "   145     0.002813   0.231748                                  \n",
      "   146     0.003915   0.231365                                  \n",
      "   147     0.002603   0.229424                                  \n",
      "   148     0.001779   0.229445                                  \n",
      "   149     0.001485   0.228959                                  \n",
      "   150     0.00255    0.23124                                   \n",
      "   151     0.003696   0.230791                                  \n",
      "   152     0.002627   0.229704                                  \n",
      "   153     0.001649   0.229374                                  \n",
      "   154     0.001478   0.229023                                  \n",
      "   155     0.002599   0.229501                                  \n",
      "   156     0.003577   0.229607                                  \n",
      "   157     0.002238   0.229552                                  \n",
      "   158     0.001657   0.230066                                  \n",
      "   159     0.001651   0.229495                                  \n",
      "   160     0.002575   0.232186                                  \n",
      "   161     0.00346    0.230728                                  \n",
      "   162     0.002279   0.229704                                  \n",
      "   163     0.001528   0.229245                                  \n",
      "   164     0.001463   0.229284                                  \n",
      "   165     0.002482   0.230295                                  \n",
      "   166     0.00363    0.231411                                  \n",
      "   167     0.002494   0.231571                                  \n",
      "   168     0.001528   0.229966                                  \n",
      "   169     0.001432   0.22994                                   \n",
      "   170     0.002641   0.231675                                  \n",
      "   171     0.003201   0.231732                                  \n",
      "   172     0.002171   0.23159                                   \n",
      "   173     0.001665   0.231029                                  \n",
      "   174     0.00143    0.230715                                  \n",
      "   175     0.00248    0.234332                                  \n",
      "   176     0.0033     0.231744                                  \n",
      "   177     0.002218   0.232666                                  \n",
      "   178     0.001609   0.230114                                  \n",
      "   179     0.001332   0.23011                                   \n",
      "   180     0.002778   0.230751                                  \n",
      "   181     0.003314   0.233397                                  \n",
      "   182     0.002186   0.231485                                  \n",
      "   183     0.001607   0.230793                                  \n",
      "   184     0.001369   0.23087                                   \n",
      "   185     0.002491   0.233055                                  \n",
      "   186     0.003311   0.233533                                  \n",
      "   187     0.002151   0.23178                                   \n",
      "   188     0.001572   0.231498                                  \n",
      "   189     0.001317   0.231662                                  \n",
      "   190     0.002229   0.232248                                  \n",
      "   191     0.002872   0.23357                                   \n",
      "   192     0.002069   0.231838                                  \n",
      "   193     0.001592   0.232722                                  \n",
      "   194     0.001366   0.232437                                  \n",
      "   195     0.002286   0.233893                                  \n",
      "   196     0.002777   0.234988                                  \n",
      "   197     0.001981   0.233293                                  \n",
      "   198     0.001563   0.232545                                  \n",
      "   199     0.001274   0.232198                                  \n",
      "   200     0.002099   0.231753                                  \n",
      "   201     0.002993   0.232927                                  \n",
      "   202     0.002093   0.23223                                   \n",
      "   203     0.001483   0.232273                                  \n",
      "   204     0.001236   0.232335                                  \n",
      "   205     0.002074   0.231936                                  \n",
      "   206     0.003067   0.232748                                  \n",
      "   207     0.002156   0.233138                                  \n",
      "   208     0.001551   0.232969                                  \n",
      "   209     0.001316   0.232761                                  \n",
      "   210     0.002128   0.234761                                  \n",
      "   211     0.003179   0.238631                                  \n",
      "   212     0.002093   0.235047                                  \n",
      "   213     0.001617   0.234097                                  \n",
      "   214     0.001293   0.233534                                  \n",
      "   215     0.002055   0.233795                                  \n",
      "   216     0.002533   0.235155                                  \n",
      "   217     0.001963   0.235111                                  \n",
      "   218     0.001355   0.233068                                  \n",
      "   219     0.001264   0.232986                                  \n",
      "   220     0.002474   0.233585                                  \n",
      "   221     0.002753   0.23593                                   \n",
      "   222     0.001941   0.233421                                  \n",
      "   223     0.001564   0.233833                                  \n",
      "   224     0.001186   0.233211                                  \n",
      "   225     0.00242    0.235779                                  \n",
      "   226     0.002685   0.236404                                  \n",
      "   227     0.001886   0.236305                                  \n",
      "   228     0.001643   0.235108                                  \n",
      "   229     0.001287   0.234657                                  \n",
      "   230     0.002114   0.235065                                  \n",
      "   231     0.00279    0.235612                                  \n",
      "   232     0.001914   0.234289                                  \n",
      "   233     0.001667   0.233976                                  \n",
      "   234     0.001295   0.233955                                  \n",
      "   235     0.0021     0.234167                                  \n",
      "   236     0.00257    0.234738                                  \n",
      "   237     0.002253   0.235211                                  \n",
      "   238     0.001386   0.235054                                  \n",
      "   239     0.001281   0.234574                                  \n",
      "   240     0.002039   0.234555                                  \n",
      "   241     0.002711   0.236369                                  \n",
      "   242     0.001786   0.236003                                  \n",
      "   243     0.001507   0.235811                                  \n",
      "   244     0.001237   0.235227                                  \n",
      "   245     0.002046   0.235699                                  \n",
      "   246     0.002528   0.236873                                  \n",
      "   247     0.001933   0.235127                                  \n",
      "   248     0.001376   0.233751                                  \n",
      "   249     0.001297   0.234083                                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.23408])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets fit the model, 50 epochs with restarts\n",
    "m.fit(lr, 50, cycle_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.load('mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=m.predict_with_targs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test=m.predict(is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_test['rating_pred'] = pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162965</th>\n",
       "      <td>4.32</td>\n",
       "      <td>4.679566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25625</th>\n",
       "      <td>2.81</td>\n",
       "      <td>2.713656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98341</th>\n",
       "      <td>4.08</td>\n",
       "      <td>4.300858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138086</th>\n",
       "      <td>3.18</td>\n",
       "      <td>3.726358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>4.19</td>\n",
       "      <td>4.097975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59636</th>\n",
       "      <td>3.77</td>\n",
       "      <td>3.660269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7638</th>\n",
       "      <td>4.59</td>\n",
       "      <td>4.699990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40669</th>\n",
       "      <td>4.58</td>\n",
       "      <td>4.631374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40279</th>\n",
       "      <td>4.50</td>\n",
       "      <td>3.936749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83116</th>\n",
       "      <td>4.42</td>\n",
       "      <td>4.038456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128454</th>\n",
       "      <td>2.81</td>\n",
       "      <td>4.278637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59616</th>\n",
       "      <td>4.59</td>\n",
       "      <td>4.484548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25604</th>\n",
       "      <td>4.04</td>\n",
       "      <td>4.130730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30209</th>\n",
       "      <td>4.85</td>\n",
       "      <td>4.432734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148310</th>\n",
       "      <td>4.27</td>\n",
       "      <td>4.098301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19666</th>\n",
       "      <td>2.81</td>\n",
       "      <td>3.004476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114233</th>\n",
       "      <td>4.64</td>\n",
       "      <td>4.021167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96173</th>\n",
       "      <td>4.81</td>\n",
       "      <td>4.500663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114941</th>\n",
       "      <td>4.12</td>\n",
       "      <td>4.128883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52096</th>\n",
       "      <td>2.92</td>\n",
       "      <td>4.208752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29780</th>\n",
       "      <td>3.46</td>\n",
       "      <td>4.158983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46057</th>\n",
       "      <td>4.85</td>\n",
       "      <td>4.868610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82242</th>\n",
       "      <td>3.81</td>\n",
       "      <td>3.948500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97090</th>\n",
       "      <td>4.77</td>\n",
       "      <td>4.193871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55166</th>\n",
       "      <td>4.50</td>\n",
       "      <td>4.163215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29545</th>\n",
       "      <td>4.23</td>\n",
       "      <td>4.522957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14593</th>\n",
       "      <td>3.85</td>\n",
       "      <td>4.124960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84903</th>\n",
       "      <td>4.09</td>\n",
       "      <td>4.668355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11412</th>\n",
       "      <td>4.41</td>\n",
       "      <td>4.108578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47964</th>\n",
       "      <td>3.23</td>\n",
       "      <td>3.525144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158379</th>\n",
       "      <td>3.64</td>\n",
       "      <td>4.086624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58608</th>\n",
       "      <td>3.77</td>\n",
       "      <td>3.764271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61046</th>\n",
       "      <td>4.38</td>\n",
       "      <td>4.263293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17921</th>\n",
       "      <td>3.77</td>\n",
       "      <td>4.226832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>4.54</td>\n",
       "      <td>4.630690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140368</th>\n",
       "      <td>4.41</td>\n",
       "      <td>4.657820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160674</th>\n",
       "      <td>2.82</td>\n",
       "      <td>3.306581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51604</th>\n",
       "      <td>4.73</td>\n",
       "      <td>4.298995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152116</th>\n",
       "      <td>3.73</td>\n",
       "      <td>4.101515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77378</th>\n",
       "      <td>4.33</td>\n",
       "      <td>4.523113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23938</th>\n",
       "      <td>4.35</td>\n",
       "      <td>4.081638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58386</th>\n",
       "      <td>3.54</td>\n",
       "      <td>3.764878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138809</th>\n",
       "      <td>4.77</td>\n",
       "      <td>4.240856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14272</th>\n",
       "      <td>4.82</td>\n",
       "      <td>4.664147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137885</th>\n",
       "      <td>2.86</td>\n",
       "      <td>2.910667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168654</th>\n",
       "      <td>4.23</td>\n",
       "      <td>4.217798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169613</th>\n",
       "      <td>4.05</td>\n",
       "      <td>4.305217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144831</th>\n",
       "      <td>3.64</td>\n",
       "      <td>3.950181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>3.73</td>\n",
       "      <td>3.883457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148134</th>\n",
       "      <td>4.23</td>\n",
       "      <td>3.701220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142802</th>\n",
       "      <td>3.73</td>\n",
       "      <td>4.194781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15886</th>\n",
       "      <td>4.69</td>\n",
       "      <td>4.620411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78870</th>\n",
       "      <td>4.04</td>\n",
       "      <td>3.792688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97383</th>\n",
       "      <td>4.46</td>\n",
       "      <td>4.611584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11187</th>\n",
       "      <td>3.81</td>\n",
       "      <td>3.809236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94792</th>\n",
       "      <td>4.62</td>\n",
       "      <td>4.392142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21287</th>\n",
       "      <td>2.81</td>\n",
       "      <td>2.904524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59472</th>\n",
       "      <td>4.69</td>\n",
       "      <td>4.496926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50033</th>\n",
       "      <td>3.85</td>\n",
       "      <td>3.749256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14990</th>\n",
       "      <td>4.00</td>\n",
       "      <td>3.831181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43836 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rating  rating_pred\n",
       "162965    4.32     4.679566\n",
       "25625     2.81     2.713656\n",
       "98341     4.08     4.300858\n",
       "138086    3.18     3.726358\n",
       "348       4.19     4.097975\n",
       "59636     3.77     3.660269\n",
       "7638      4.59     4.699990\n",
       "40669     4.58     4.631374\n",
       "40279     4.50     3.936749\n",
       "83116     4.42     4.038456\n",
       "128454    2.81     4.278637\n",
       "59616     4.59     4.484548\n",
       "25604     4.04     4.130730\n",
       "30209     4.85     4.432734\n",
       "148310    4.27     4.098301\n",
       "19666     2.81     3.004476\n",
       "114233    4.64     4.021167\n",
       "96173     4.81     4.500663\n",
       "114941    4.12     4.128883\n",
       "52096     2.92     4.208752\n",
       "29780     3.46     4.158983\n",
       "46057     4.85     4.868610\n",
       "82242     3.81     3.948500\n",
       "97090     4.77     4.193871\n",
       "55166     4.50     4.163215\n",
       "29545     4.23     4.522957\n",
       "14593     3.85     4.124960\n",
       "84903     4.09     4.668355\n",
       "11412     4.41     4.108578\n",
       "47964     3.23     3.525144\n",
       "...        ...          ...\n",
       "158379    3.64     4.086624\n",
       "58608     3.77     3.764271\n",
       "61046     4.38     4.263293\n",
       "17921     3.77     4.226832\n",
       "3793      4.54     4.630690\n",
       "140368    4.41     4.657820\n",
       "160674    2.82     3.306581\n",
       "51604     4.73     4.298995\n",
       "152116    3.73     4.101515\n",
       "77378     4.33     4.523113\n",
       "23938     4.35     4.081638\n",
       "58386     3.54     3.764878\n",
       "138809    4.77     4.240856\n",
       "14272     4.82     4.664147\n",
       "137885    2.86     2.910667\n",
       "168654    4.23     4.217798\n",
       "169613    4.05     4.305217\n",
       "144831    3.64     3.950181\n",
       "1626      3.73     3.883457\n",
       "148134    4.23     3.701220\n",
       "142802    3.73     4.194781\n",
       "15886     4.69     4.620411\n",
       "78870     4.04     3.792688\n",
       "97383     4.46     4.611584\n",
       "11187     3.81     3.809236\n",
       "94792     4.62     4.392142\n",
       "21287     2.81     2.904524\n",
       "59472     4.69     4.496926\n",
       "50033     3.85     3.749256\n",
       "14990     4.00     3.831181\n",
       "\n",
       "[43836 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_test[['rating','rating_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = sqrt(mean_squared_error(joined_test['rating'], joined_test['rating_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48265219993287534"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we achieved a root mean squared error of .48\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_record = df_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user             18277\n",
       "course              82\n",
       "category            11\n",
       "rating            4.32\n",
       "job                 12\n",
       "institution          0\n",
       "state               57\n",
       "rating_pred    4.67957\n",
       "Name: 162965, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = test_record[cat_vars].values.astype(np.int64)[None]\n",
    "contin = test_record.drop(cat_vars).values.astype(np.float32)[None]\n",
    "\n",
    "#Prediction\n",
    "model = m.model\n",
    "model.eval()\n",
    "prediction = to_np(model(V(cat),[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.679566"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
